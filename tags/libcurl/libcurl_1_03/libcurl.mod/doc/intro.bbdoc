<a href="http://curl.haxx.se/libcurl/"><img src="libcurl_logo.png" align="right" border="0" width="138" height="64"></a>
<p><a href="http://curl.haxx.se/libcurl/">libcurl</a> is a free and easy-to-use client-side URL transfer library, supporting FTP, FTPS, HTTP, HTTPS, SCP, SFTP, TFTP, TELNET, DICT, FILE and LDAP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos4), file transfer resume, http proxy tunneling and more!
</p>
<p>This BlitzMax module wraps up the functionality of libcurl, making it easy to accomplish internet-based tasks with very little effort.</p>
<h3>Which is best for you?</h3>
<p>
There are two libcurl modules available. One with SSH snd SSL support, one without. This is done because the SSL support requires extra client libraries, something which may be too much of a hassle to have users install if you don't need it.
</p>
<p>
SSL support enables the use of secure protocols like HTTPS and FTPS.
</p>
<p>
This is the <b>non-SSL</b> module.
</p>
<h2>Examples</h2>
<p>
The following is a list of some simple examples showing a small portion of the features available with the module.
</p>
<ul>
<li><a href="../examples/ex_01.bmx">ex_01.bmx</a> - Fetches a URL and outputs to the console.</li>
<li><a href="../examples/ex_02.bmx">ex_02.bmx</a> - Fetches a URL, indicating progress.</li>
<li><a href="../examples/ex_03.bmx">ex_03.bmx</a> - FTP directory list, using custom Stream and debug callback.</li>
<li><a href="../examples/ex_04.bmx">ex_04.bmx</a> - FTP directory list and parse (using BaH.FTPParser).</li>
<li><a href="../examples/ex_05.bmx">ex_05.bmx</a> - Multi-interface, retrieving 3 URLs simultaneously.</li>
<li><a href="../examples/ex_06.bmx">ex_06.bmx</a> - Lists cookies retrieved from a website.</li>
<li><a href="../examples/ex_07.bmx">ex_07.bmx</a> - Setting User-Agent and Referrer: headers.</li>
</ul>
<h2>A guide to libcurl - The Easy Interface</h2>
<p>This guide is based on the libcurl tutorial that comes with the curl distribution, and has been changed where appropriate for use with the BlitzMax libcURL module.
</p>
<p>
The easy interface is synchronous, meaning that you must wait for an action to complete before control is returned to your program. The asynchronous method uses what is called the multi-interface. More about that interface, what it is targeted for and how to use it is detailed <a href="#multiinterface">here</a>. You still need to understand the easy interface first, so please continue reading for better understanding.
</p>
<p>
To use the easy interface, you must first create yourself a <a href="#TCurlEasy">TCurlEasy</a>. You need one of these for each easy session you want to perform.
</p>
<p>
Get an easy handle with
<pre>
 easyhandle = TCurlEasy.Create();
</pre>
It returns a <a href="#TCurlEasy">TCurlEasy</a> object. Using that you proceed to the next step: setting up your preferred actions. A handle is just a logic entity for the upcoming transfer or series of transfers.
</p>
<p>
You set properties and options for this handle using one of the setOpt methods, or one of the other convenience methods (see <a href="#TCurlEasy">TCurlEasy</a> for more specific details). They control how the subsequent transfer or transfers will be made. Options remain set in the handle until set again to something different. Alas, multiple requests using the same handle will use the same options.
</p>
<p>
One of the most basic properties to set in the handle is the URL. You set your preferred URL to transfer with <a href="#CURLOPT_URL">CURLOPT_URL</a> in a manner similar to:
<pre>
  curl.setOptString(CURLOPT_URL, "http://domain.com/")
</pre>
Let's assume for a while that you want to receive data as the URL identifies a remote resource you want to get here. Since you write a sort of application that needs this transfer, I assume that you would like to get the data passed to you directly instead of simply getting it passed to stdout. So, you write your own function that matches this prototype: 
<pre>
  writeFunction(buffer:Byte Ptr, size:Int, data:Object)
</pre>
You tell libcurl to pass all data to this function by issuing a call to <a href="#setWriteCallback">setWriteCallback</a>.
</p>
<p>
The <b>data</b> parameter allows you to easily pass local data between your application and the function that gets invoked by libcurl. libcurl itself won't touch the data you pass here.
</p>
<p>
libcurl offers its own default internal callback that'll take care of the data if you don't set the callback. It will then simply output the received data to stdout.
</p>
<p>
You can also pass in a Stream for libcurl to use to write the data to with the <a href="#setWriteStream">setWriteStream</a> method. You will have to manage the opening and closing of the stream yourself. libcurl assumes it is already opened when it attempts to use it.
</p>
<p>
There are of course many more options you can set, and we'll get back to a few of them later. Let's instead continue to the actual transfer:
<pre>
  success = curl.perform()
</pre>
<a href="#perform">perform</a> will connect to the remote site, do the necessary commands and receive the transfer. Whenever it receives data, it calls the callback function we previously set. The function may get one byte at a time, or it may get many kilobytes at once. libcurl delivers as much as possible as often as possible. Your callback function should return the number of bytes it "took care of". If that is not the exact same amount of bytes that was passed to it, libcurl will abort the operation and return with an error code.
</p>
<p>
When the transfer is complete, the function returns a return code that informs you if it succeeded in its mission or not. If a return code isn't enough for you, you can call <a href="#CurlError">CurlError</a> passing the error code, to receive the error text as a String.
</p>
<p>
If you then want to transfer another file, the handle is ready to be used again. Mind you, it is even preferred that you re-use an existing handle if you intend to make another transfer. libcurl will then attempt to re-use the previous connection. 
</p>
<h3>When It Doesn't Work</h3>
<p>
There will always be times when the transfer fails for some reason. You might have set the wrong libcurl option or misunderstood what the libcurl option actually does, or the remote server might return non-standard replies that confuse the library which then confuses your program.
</p>
<p>
There's one golden rule when these things occur: set the <a href="#CURLOPT_VERBOSE">CURLOPT_VERBOSE</a> option to True. It'll cause the library to spew out the entire protocol details it sends, some internal info and some received protocol data as well (especially when using FTP). If you're using HTTP, adding the headers in the received output to study is also a clever way to get a better understanding why the server behaves the way it does. Include headers in the normal body output with <a href="#CURLOPT_HEADER">CURLOPT_HEADER</a> set True.
</p>
<p>
If <a href="#CURLOPT_VERBOSE">CURLOPT_VERBOSE</a> is not enough, you increase the level of debug data your application receive by using the <a href="#setDebugCallback">setDebugCallback</a> method.
</p>
<p>
Getting some in-depth knowledge about the protocols involved is never wrong, and if you're trying to do funny things, you might very well understand libcurl and how to use it better if you study the appropriate RFC documents at least briefly. 
</p>
<h3>Upload Data to a Remote Site</h3>
<p>
libcurl tries to keep a protocol independent approach to most transfers, thus uploading to a remote FTP site is very similar to uploading data to a HTTP server with a PUT request.
</p>
<p>
Of course, first you either create a <a href="#TCurlEasy">TCurlEasy</a> handle or you re-use one existing one. Then you set the URL to operate on just like before. This is the remote URL, that we now will upload.
</p>
<p>
Since we write an application, we most likely want libcurl to get the upload data by asking us for it. To make it do that, we set the read callback and the custom object libcurl will pass to our read callback. The read callback should have a prototype similar to:
<pre>
  readFunction(buffer:Byte Ptr, size:Int, data:Object)
</pre>
Where buffer is the pointer to a buffer we fill in with data to upload and size is the size of the buffer and therefore also the maximum amount of data we can return to libcurl in this call. The 'data' object is the custom object we set to point to a type of ours to pass private data between the application and the callback.
<pre>
  curl.setReadCallback(myReadFunction, myData)
</pre>
Tell libcurl that we want to upload:
<pre>
  curl.setOptInt(CURLOPT_UPLOAD, True)
</pre>
</p>
<p>
A few protocols won't behave properly when uploads are done without any prior knowledge of the expected file size. So, set the upload file size using the <a href="#CURLOPT_INFILESIZE_LARGE">CURLOPT_INFILESIZE_LARGE</a> for all known file sizes like this :
<pre>
  ' in this example, size must be an Long variable
  curl.setOptLong(CURLOPT_INFILESIZE_LARGE, size)
</pre>
</p>
<p>
When you call <a href="#perform">perform</a> this time, it'll perform all the necessary operations and when it has invoked the upload it'll call your supplied callback to get the data to upload. The program should return as much data as possible in every invoke, as that is likely to make the upload perform as fast as possible. The callback should return the number of bytes it wrote in the buffer. Returning 0 will signal the end of the upload.
</p>
<p>
You can also use <a href="#setReadStream">setReadStream</a> instead, providing a specific Stream for libcurl to read from. You will have to manage the opening and closing of the stream yourself. libcurl assumes it is already opened when it attempts to use it.
</p>
<h3>Passwords</h3>
<p>
Many protocols use or even require that user name and password are provided to be able to download or upload the data of your choice. libcurl offers several ways to specify them.
</p>
<p>
Most protocols support that you specify the name and password in the URL itself. libcurl will detect this and use them accordingly. This is written like this:
<pre>
 protocol://user:password@example.com/path/
</pre>
If you need any odd letters in your user name or password, you should enter them URL encoded, as %XX where XX is a two-digit hexadecimal number.
</p>
<p>
libcurl also provides options to set various passwords. The user name and password as shown embedded in the URL can instead get set with the <a href="#CURLOPT_USERPWD">CURLOPT_USERPWD</a> option. The argument passed to libcurl should be a char * to a string in the format "user:password:". In a manner like this:
<pre>
 curl.setOptString(CURLOPT_USERPWD, "myname:thesecret")
</pre>
</p>
<p>
Another case where name and password might be needed at times, is for those users who need to authenticate themselves to a proxy they use. libcurl offers another option for this, the <a href="#CURLOPT_PROXYUSERPWD">CURLOPT_PROXYUSERPWD</a>. It is used quite similar to the <a href="#CURLOPT_USERPWD">CURLOPT_USERPWD</a> option like this:
<pre>
 curl.setOptString(CURLOPT_PROXYUSERPWD, "myname:thesecret")
</pre>
</p>
<p>
There's a long time unix "standard" way of storing ftp user names and passwords, namely in the $HOME/.netrc file. The file should be made private so that only the user may read it (see also the "Security Considerations" section), as it might contain the password in plain text. libcurl has the ability to use this file to figure out what set of user name and password to use for a particular host. As an extension to the normal functionality, libcurl also supports this file for non-FTP protocols such as HTTP. To make curl use this file, use the <a href="#CURLOPT_NETRC">CURLOPT_NETRC</a> option:
<pre>
 curl._setOptString(CURLOPT_NETRC, True)
</pre>
And a very basic example of how such a .netrc file may look like:
<pre>
 machine myhost.mydomain.com  login userlogin  password secretword
</pre>
</p>
<p>
All these examples have been cases where the password has been optional, or at least you could leave it out and have libcurl attempt to do its job without it. There are times when the password isn't optional, like when you're using an SSL private key for secure transfers.
</p>
<p>
To pass the known private key password to libcurl:
<pre>
 curl.setOptString(CURLOPT_SSLKEYPASSWD, "keypassword")
</pre>
</p>
<h3>HTTP Authentication</h3>
<p>
The previous section showed how to set user name and password for getting URLs that require authentication. When using the HTTP protocol, there are many different ways a client can provide those credentials to the server and you can control what way libcurl will (attempt to) use. The default HTTP authentication method is called 'Basic', which is sending the name and password in clear-text in the HTTP request, base64-encoded. This is insecure.
</p>
<p>
At the time of this writing libcurl can be built to use: Basic, Digest, NTLM, Negotiate, GSS-Negotiate and SPNEGO. You can tell libcurl which one to use with <a href="#CURLOPT_HTTPAUTH">CURLOPT_HTTPAUTH</a> as in:
<pre>
 curl.setOptInt(CURLOPT_HTTPAUTH, CURLAUTH_DIGEST)
</pre>
And when you send authentication to a proxy, you can also set authentication type the same way but instead with <a href="#CURLOPT_PROXYAUTH">CURLOPT_PROXYAUTH</a>:
<pre>
 curl.setOptInt(CURLOPT_PROXYAUTH, CURLAUTH_NTLM)
</pre>
Both these options allow you to set multiple types (by ORing them together), to make libcurl pick the most secure one out of the types the server/proxy claims to support. This method does however add a round-trip since libcurl must first ask the server what it supports:
<pre>
 curl.setOptInt(CURLOPT_HTTPAUTH,  CURLAUTH_DIGEST | CURLAUTH_BASIC)
</pre>
For convenience, you can use the <a href="#CURLAUTH_ANY">CURLAUTH_ANY</a> flag (instead of a list with specific types) which allows libcurl to use whatever method it wants.
</p>
<p>
When asking for multiple types, libcurl will pick the available one it considers "best" in its own internal order of preference. 
</p>
<h3>HTTP POSTing</h3>
<p>
There are always many questions regarding how to issue HTTP POSTs with libcurl the proper way, so section will include examples using both different versions of HTTP POST that libcurl supports.
</p>
<p>
The first version is the simple POST, the most common version, that most HTML pages using the <form> tag uses. We provide a String of the data and tell libcurl to post it all to the remote site:
<pre>
  Local data:String = "name=daniel&project=curl"
  curl.setOptString(CURLOPT_POSTFIELDS, data)
  curl.setOptString(CURLOPT_URL, "http://posthere.com/")

  curl.perform() ' post away!
</pre>
Simple enough, huh? Since you set the POST options with the <a href="#CURLOPT_POSTFIELDS">CURLOPT_POSTFIELDS</a>, this automatically switches the handle to use POST in the upcoming request.
</p>
<p>
Ok, so what if you want to post binary data that also requires you to set the Content-Type: header of the post? Well, binary posts prevents libcurl from being able to do strlen() on the data to figure out the size, so therefore we must tell libcurl the size of the post data. Setting headers in libcurl requests are done in a generic way, by building a list of our own headers and then passing that list to libcurl.
<pre>
 local headers:String[] = ["Content-Type: text/xml"]

 ' post binary data
 curl.setOptBytePtr(CURLOPT_POSTFIELDS, binaryptr)

 ' set the size of the postfields data
 curl.setOptInt(CURLOPT_POSTFIELDSIZE, 23)

 ' pass our list of custom made headers
 curl.httpHeader(headers)

 curl.perform() ' post away!

 curl.freeLists() ' free the header list
</pre>
</p>
<p>
While the simple examples above cover the majority of all cases where HTTP POST operations are required, they don't do multi-part formposts. Multi-part formposts were introduced as a better way to post (possibly large) binary data and was first documented in the RFC1867. They're called multi-part because they're built by a chain of parts, each being a single unit. Each part has its own name and contents. You can in fact create and post a multi-part formpost with the regular libcurl POST support described above, but that would require that you build a formpost yourself and provide to libcurl. To make that easier, the libcurl module provides <a href="#TCurlFormData">TCurlFormData</a>, a special object designed specifically for creating them. Using this, you add parts to the form. When you're done adding parts, you post the whole form via <a href="#httpPost">httpPost</a>. (<i>Remember to keep the object around for as long as the post takes, so that it is not garbage-collect too early, otherwise strange things may happen!</i>)
</p>
<p>
The following example sets two simple text parts with plain textual contents, and then a file with binary contents and upload the whole thing.
<pre>
  Local form:TCurlFormData = TCurlFormData.Create()
  
  form.addContent("name", "daniel")
  form.addContent("project", "curl")
  form.addFileContent("logotype-image", "curl.png")

  ' Set the form info
  curl.httpPost(form)

  curl.perform() ' post away!
</pre>
</p>
<p>  
Multipart formposts are chains of parts using MIME-style separators and headers. It means that each one of these separate parts get a few headers set that describe the individual content-type, size etc. To enable your application to handicraft this formpost even more, libcurl allows you to supply your own set of custom headers to such an individual form part. You can of course supply headers to as many parts you like, but this little example will show how you set headers to one specific part when you add that to the post handle:
<pre>
  Local headers:String[] = ["Content-Type: text/xml"]
  
  form.addFileContent("logotype-image", "curl.xml", Null, headers)
  
  curl.perform() ' post away!
</pre>
And, for a Content-Type header, you could as easily have passed a String as the third parameter of <a href="#addFileContent">addFileContent</a> instead.
</p>
<p>
Since all options on an easyhandle are "sticky", they remain the same until changed even if you do call <a href="#perform">perform</a>, you may need to tell curl to go back to a plain GET request if you intend to do such a one as your next request. You force an easyhandle to back to GET by using the <a href="#CURLOPT_HTTPGET>CURLOPT_HTTPGET</a> option:
<pre>
  curl.setOptInt(CURLOPT_HTTPGET, True)
</pre>
</p>
<p>
Just setting <a href="#CURLOPT_POSTFIELDS>CURLOPT_POSTFIELDS</a> to "" or Null will *not* stop libcurl from doing a POST. It will just make it POST without any data to send!
</p>
<h3>Showing Progress</h3>
<p>
For historical and traditional reasons, libcurl has a built-in progress meter that can be switched on and then makes it presents a progress meter in your terminal.
</p>
<p>
Switch on the progress meter by, oddly enough, set <a href="#CURLOPT_NOPROGRESS">CURLOPT_NOPROGRESS</a> to False. This option is set to True by default.
</p>
<p>
For most applications however, the built-in progress meter is useless and what instead is interesting is the ability to specify a progress callback. The function pointer you pass to libcurl will then be called on irregular intervals with information about the current transfer.
</p>
<p>
Set the progress callback by using <a href="#setProgressCallback">setProgressCallback</a>. And pass a pointer to a function that matches this prototype:
<pre>
  progressFunction:Int(data:Object, dltotal:Double, dlnow:Double, ultotal:Double, ulnow:Double)
</pre>
</p>
<p>
If any of the input arguments is unknown, a 0 will be passed. The first argument, 'data' is an optional object you pass to libcurl as the second parameter to <a href="#setProgressCallback">setProgressCallback</a>. libcurl won't touch it. 
</p>
<h3>Proxies</h3>
<p>
What "proxy" means according to Merriam-Webster: "a person authorized to act for another" but also "the agency, function, or office of a deputy who acts as a substitute for another".
</p>
<p>
Proxies are exceedingly common these days. Companies often only offer Internet access to employees through their proxies. Network clients or user-agents ask the proxy for documents, the proxy does the actual request and then it returns them.
</p>
<p>
libcurl supports SOCKS and HTTP proxies. When a given URL is wanted, libcurl will ask the proxy for it instead of trying to connect to the actual host identified in the URL.
</p>
<p>
If you're using a SOCKS proxy, you may find that libcurl doesn't quite support all operations through it.
</p>
<p>
For HTTP proxies: the fact that the proxy is a HTTP proxy puts certain restrictions on what can actually happen. A requested URL that might not be a HTTP URL will be still be passed to the HTTP proxy to deliver back to libcurl. This happens transparently, and an application may not need to know. I say "may", because at times it is very important to understand that all operations over a HTTP proxy is using the HTTP protocol. For example, you can't invoke your own custom FTP commands or even proper FTP directory listings.
</p>
<blockquote>
<h3>Proxy Options</h3>
<p>
To tell libcurl to use a proxy at a given port number:
<pre>
  curl.setOptString(CURLOPT_PROXY, "proxy-host.com:8080")
</pre>
Some proxies require user authentication before allowing a request, and you pass that information similar to this:
<pre>
 curl.setOptString(CURLOPT_PROXYUSERPWD, "user:password")
</pre>
If you want to, you can specify the host name only in the <a href="#CURLOPT_PROXY">CURLOPT_PROXY</a> option, and set the port number separately with <a href="#CURLOPT_PROXYPORT">CURLOPT_PROXYPORT</a>.
</p>
<p>
Tell libcurl what kind of proxy it is with <a href="#CURLOPT_PROXYTYPE">CURLOPT_PROXYTYPE</a> (if not, it will default to assume a HTTP proxy):
<pre>
  curl.setOptInt(easyhandle, CURLOPT_PROXYTYPE, CURLPROXY_SOCKS4)
</pre>
</p>
<h3>Environment Variables</h3>
<p>
libcurl automatically checks and uses a set of environment variables to know what proxies to use for certain protocols. The names of the variables are following an ancient de facto standard and are built up as "[protocol]_proxy" (note the lower casing). Which makes the variable 'http_proxy' checked for a name of a proxy to use when the input URL is HTTP. Following the same rule, the variable named 'ftp_proxy' is checked for FTP URLs. Again, the proxies are always HTTP proxies, the different names of the variables simply allows different HTTP proxies to be used.
</p>
<p>
The proxy environment variable contents should be in the format "[protocol://][user:password@]machine[:port]". Where the protocol:// part is simply ignored if present (so http://proxy and bluerk://proxy will do the same) and the optional port number specifies on which port the proxy operates on the host. If not specified, the internal default port number will be used and that is most likely *not* the one you would like it to be.
</p>
<p>
There are two special environment variables. 'all_proxy' is what sets proxy for any URL in case the protocol specific variable wasn't set, and 'no_proxy' defines a list of hosts that should not use a proxy even though a variable may say so. If 'no_proxy' is a plain asterisk ("*") it matches all hosts.
</p>
<p>
To explicitly disable libcurl's checking for and using the proxy environment variables, set the proxy name to "" - an empty string - with <a href="#CURLOPT_PROXY">CURLOPT_PROXY</a>.
</p>
<h3>SSL and Proxies</h3>
<p>
SSL is for secure point-to-point connections. This involves strong encryption and similar things, which effectively makes it impossible for a proxy to operate as a "man in between" which the proxy's task is, as previously discussed. Instead, the only way to have SSL work over a HTTP proxy is to ask the proxy to tunnel trough everything without being able to check or fiddle with the traffic.
</p>
<p>
Opening an SSL connection over a HTTP proxy is therefore a matter of asking the proxy for a straight connection to the target host on a specified port. This is made with the HTTP request CONNECT. ("please mr proxy, connect me to that remote host").
</p>
<p>
Because of the nature of this operation, where the proxy has no idea what kind of data that is passed in and out through this tunnel, this breaks some of the very few advantages that come from using a proxy, such as caching. Many organizations prevent this kind of tunneling to other destination port numbers than 443 (which is the default HTTPS port number).
</p>
<h3>Tunneling Through Proxy</h3>
<p>
As explained above, tunneling is required for SSL to work and often even restricted to the operation intended for SSL; HTTPS.
</p>
<p>
This is however not the only time proxy-tunneling might offer benefits to you or your application.
</p>
<p>
As tunneling opens a direct connection from your application to the remote machine, it suddenly also re-introduces the ability to do non-HTTP operations over a HTTP proxy. You can in fact use things such as FTP upload or FTP custom commands this way.
</p>
<p>
Again, this is often prevented by the administrators of proxies and is rarely allowed.
</p>
<p>
Tell libcurl to use proxy tunneling like this:
<pre>
  curl.setOptInt(CURLOPT_HTTPPROXYTUNNEL, True)
</pre>
<p>
In fact, there might even be times when you want to do plain HTTP operations using a tunnel like this, as it then enables you to operate on the remote server instead of asking the proxy to do so. libcurl will not stand in the way for such innovative actions either!
</p>
<h3>Proxy Auto-Config</h3>
<p>
Netscape first came up with this. It is basically a web page (usually using a .pac extension) with a javascript that when executed by the browser with the requested URL as input, returns information to the browser on how to connect to the URL. The returned information might be "DIRECT" (which means no proxy should be used), "PROXY host:port" (to tell the browser where the proxy for this particular URL is) or "SOCKS host:port" (to direct the browser to a SOCKS proxy).
</p>
<p>
libcurl has no means to interpret or evaluate javascript and thus it doesn't support this. If you get yourself in a position where you face this nasty invention, the following advice have been mentioned and used in the past:
<ul>
<li>Depending on the javascript complexity, write up a script that translates it to another language and execute that.</li>
<li>Read the javascript code and rewrite the same logic in another language.</li>
<li>Implement a javascript interpreted, people have successfully used the Mozilla javascript engine in the past.</li>
<li>Ask your admins to stop this, for a static proxy setup or similar.</li>
</ul>
</p>
</blockquote>
<h3>Persistence Is The Way to Happiness</h3>
<p>
Re-cycling the same easy handle several times when doing multiple requests is the way to go.
</p>
<p>
After each single <a href="#perform">perform</a> operation, libcurl will keep the connection alive and open. A subsequent request using the same easy handle to the same host might just be able to use the already open connection! This reduces network impact a lot.
</p>
<p>
Even if the connection is dropped, all connections involving SSL to the same host again, will benefit from libcurl's session ID cache that drastically reduces re-connection time.
</p>
<p>
FTP connections that are kept alive saves a lot of time, as the command- response round-trips are skipped, and also you don't risk getting blocked without permission to login again like on many FTP servers only allowing N persons to be logged in at the same time.
</p>
<p>
libcurl caches DNS name resolving results, to make lookups of a previously looked up name a lot faster.
</p>
<p>
Other interesting details that improve performance for subsequent requests may also be added in the future.
</p>
<p>
Each easy handle will attempt to keep the last few connections alive for a while in case they are to be used again. You can set the size of this "cache" with the <a href="#CURLOPT_MAXCONNECTS">CURLOPT_MAXCONNECTS</a> option. Default is 5. It is very seldom any point in changing this value, and if you think of changing this it is often just a matter of thinking again.
</p>
<p>
To force your upcoming request to not use an already existing connection (it will even close one first if there happens to be one alive to the same host you're about to operate on), you can do that by setting <a href="#CURLOPT_FRESH_CONNECT">CURLOPT_FRESH_CONNECT</a> to True. In a similar spirit, you can also forbid the upcoming request to be "lying" around and possibly get re-used after the request by setting <a href="#CURLOPT_FORBID_REUSE">CURLOPT_FORBID_REUSE</a> to True.
</p>
<h3>HTTP Headers Used by libcurl</h3>
<p>
When you use libcurl to do HTTP requests, it'll pass along a series of headers automatically. It might be good for you to know and understand these ones. You can replace or remove them by using <a href="#httpHeader">httpHeader</a>.
<blockquote>
<h3>Host</h3>
<p>
This header is required by HTTP 1.1 and even many 1.0 servers and should be the name of the server we want to talk to. This includes the port number if anything but default.
</p>
<h3>Pragma</h3>
<p>
"no-cache". Tells a possible proxy to not grab a copy from the cache but to fetch a fresh one.
</p>
<h3>Accept</h3>
<p>
"*/*".
</p>
<h3>Expect</h3>
<p>
When doing POST requests, libcurl sets this header to "100-continue" to ask the server for an "OK" message before it proceeds with sending the data part of the post. If the POSTed data amount is deemed "small", libcurl will not use this header.
</p>
</blockquote>
<h3>Customizing Operations</h3>
<p>
There is an ongoing development today where more and more protocols are built upon HTTP for transport. This has obvious benefits as HTTP is a tested and reliable protocol that is widely deployed and have excellent proxy-support.
</p>
<p>
When you use one of these protocols, and even when doing other kinds of programming you may need to change the traditional HTTP (or FTP or...) manners. You may need to change words, headers or various data.
</p>
<p>
libcurl is your friend here too. 
</p>
<blockquote>
<h3>CUSTOMREQUEST</h3>
<p>
If just changing the actual HTTP request keyword is what you want, like when GET, HEAD or POST is not good enough for you, <a href="#CURLOPT_CUSTOMREQUEST">CURLOPT_CUSTOMREQUEST</a> is there for you. It is very simple to use:
<pre>
  curl.setOptString(CURLOPT_CUSTOMREQUEST, "MYOWNRUQUEST")
</pre>
</p>
<p>
When using the custom request, you change the request keyword of the actual request you are performing. Thus, by default you make GET request but you can also make a POST operation (as described before) and then replace the POST keyword if you want to. You're the boss.
</p>
<h3>Modify Headers</h3>
<p>
HTTP-like protocols pass a series of headers to the server when doing the request, and you're free to pass any amount of extra headers that you think fit. Adding headers are this easy:
<pre>
 Local headers:String[] = ["Hey-server-hey: how are you?", "X-silly-content: yes"]

 ' pass our list of custom made headers
 curl.httpHeader(headers)

 curl.perform() ' transfer http

 curl.freeLists() ' free the header list
</pre>
... and if you think some of the internally generated headers, such as Accept: or Host: don't contain the data you want them to contain, you can replace them by simply setting them too:
<pre>
 headers = ["Accept: Agent-007", "Host: munged.host.line"]
</pre>
</p>
<h3>Delete Headers</h3>
<p>
If you replace an existing header with one with no contents, you will prevent the header from being sent. Like if you want to completely prevent the "Accept:" header to be sent, you can disable it with code similar to this:
<pre>
  headers = ["Accept:"]
</pre>
Both replacing and canceling internal headers should be done with careful consideration and you should be aware that you may violate the HTTP protocol when doing so. 
</p>
<h3>Enforcing chunked transfer-encoding</h3>
<p>
By making sure a request uses the custom header "Transfer-Encoding: chunked" when doing a non-GET HTTP operation, libcurl will switch over to "chunked" upload, even though the size of the data to upload might be known. By default, libcurl usually switches over to chunked upload automatically if the upload data size is unknown.
</p>
<h3>HTTP Version</h3>
<p>
All HTTP requests includes the version number to tell the server which version we support. libcurl speak HTTP 1.1 by default. Some very old servers don't like getting 1.1-requests and when dealing with stubborn old things like that, you can tell libcurl to use 1.0 instead by doing something like this:
<pre>
  curl.setOptInt(CURLOPT_HTTP_VERSION, CURL_HTTP_VERSION_1_0)
</pre>
</p>
<h3>FTP Custom Commands</h3>
<p>
Not all protocols are HTTP-like, and thus the above may not help you when you want to make for example your FTP transfers to behave differently.
</p>
<p>
Sending custom commands to a FTP server means that you need to send the commands exactly as the FTP server expects them (RFC959 is a good guide here), and you can only use commands that work on the control-connection alone. All kinds of commands that requires data interchange and thus needs a data-connection must be left to libcurl's own judgment. Also be aware that libcurl will do its very best to change directory to the target directory before doing any transfer, so if you change directory (with CWD or similar) you might confuse libcurl and then it might not attempt to transfer the file in the correct remote directory.
</p>
<p>
A little example that deletes a given file before an operation:
<pre>
 Local commands:String[] = ["DELE file-to-remove"]

 ' pass the list of custom commands to the handle
 curl.quote(commands)

 curl.perform() ' transfer ftp data!

 curl.freeLists() ' free the header list
</pre>
</p>
<p>
If you would instead want this operation (or chain of operations) to happen <b>after</b> the data transfer took place the option you could instead call <a href="#postQuote">postQuote</a> in the exact same way.
</p>
<p>
The custom FTP command will be issued to the server in the same order they are added to the list, and if a command gets an error code returned back from the server, no more commands will be issued and libcurl will bail out with an error code (<a href="#CURLE_FTP_QUOTE_ERROR">CURLE_FTP_QUOTE_ERROR</a>). Note that if you use <a href="#quote">quote</a> to send commands before a transfer, no transfer will actually take place when a quote command has failed.
</p>
<p>
If you set the <a href="#CURLOPT_HEADER">CURLOPT_HEADER</a> to True, you will tell libcurl to get information about the target file and output "headers" about it. The headers will be in "HTTP-style", looking like they do in HTTP.
</p>
<p>
The option to enable headers or to run custom FTP commands may be useful to combine with <a href="#CURLOPT_NOBODY">CURLOPT_NOBODY</a>. If this option is set, no actual file content transfer will be performed. 
</p>
<h3>FTP Custom CUSTOMREQUEST</h3>
<p>
If you do what list the contents of a FTP directory using your own defined FTP command, <a href="#CURLOPT_CUSTOMREQUEST">CURLOPT_CUSTOMREQUEST</a> will do just that. "NLST" is the default one for listing directories but you're free to pass in your idea of a good alternative.
</p>
</blockquote>
<h3>Cookies Without Chocolate Chips</h3>
<p>
In the HTTP sense, a cookie is a name with an associated value. A server sends the name and value to the client, and expects it to get sent back on every subsequent request to the server that matches the particular conditions set. The conditions include that the domain name and path match and that the cookie hasn't become too old.
</p>
<p>
In real-world cases, servers send new cookies to replace existing one to update them. Server use cookies to "track" users and to keep "sessions".
</p>
<p>
Cookies are sent from server to clients with the header Set-Cookie: and they're sent from clients to servers with the Cookie: header.
</p>
<p>
To just send whatever cookie you want to a server, you can use <a href="#CURLOPT_COOKIE">CURLOPT_COOKIE</a> to set a cookie string like this:
<pre>
  curl.setOptString(CURLOPT_COOKIE, "name1=var1; name2=var2;")
</pre>
</p>
<p>
In many cases, that is not enough. You might want to dynamically save whatever cookies the remote server passes to you, and make sure those cookies are then use accordingly on later requests.
</p>
<p>
One way to do this, is to save all headers you receive in a plain file and when you make a request, you tell libcurl to read the previous headers to figure out which cookies to use. Set header file to read cookies from with <a href="#CURLOPT_COOKIEFILE">CURLOPT_COOKIEFILE</a>.
</p>
<p>
The <a href="#CURLOPT_COOKIEFILE">CURLOPT_COOKIEFILE</a> option also automatically enables the cookie parser in libcurl. Until the cookie parser is enabled, libcurl will not parse or understand incoming cookies and they will just be ignored. However, when the parser is enabled the cookies will be understood and the cookies will be kept in memory and used properly in subsequent requests when the same handle is used. Many times this is enough, and you may not have to save the cookies to disk at all. Note that the file you specify to <a href="#CURLOPT_COOKIEFILE">CURLOPT_COOKIEFILE</a> doesn't have to exist to enable the parser, so a common way to just enable the parser and not read able might be to use a file name you know doesn't exist.
</p>
<p>
If you rather use existing cookies that you've previously received with your Netscape or Mozilla browsers, you can make libcurl use that cookie file as input. The <a href="#CURLOPT_COOKIEFILE">CURLOPT_COOKIEFILE</a> is used for that too, as libcurl will automatically find out what kind of file it is and act accordingly.
</p>
<p>
The perhaps most advanced cookie operation libcurl offers, is saving the entire internal cookie state back into a Netscape/Mozilla formatted cookie file. We call that the cookie-jar. When you set a file name with <a href="#CURLOPT_COOKIEJAR">CURLOPT_COOKIEJAR</a>, that file name will be created and all received cookies will be stored in it when <a href="#cleanup">cleanup</a> is called. This enabled cookies to get passed on properly between multiple handles without any information getting lost.
</p>
<h3>FTP Peculiarities We Need</h3>
<p>
FTP transfers use a second TCP/IP connection for the data transfer. This is usually a fact you can forget and ignore but at times this fact will come back to haunt you. libcurl offers several different ways to custom how the second connection is being made.
</p>
<p>
libcurl can either connect to the server a second time or tell the server to connect back to it. The first option is the default and it is also what works best for all the people behind firewalls, NATs or IP-masquerading setups. libcurl then tells the server to open up a new port and wait for a second connection. This is by default attempted with EPSV first, and if that doesn't work it tries PASV instead. (EPSV is an extension to the original FTP spec and does not exist nor work on all FTP servers.)
</p>
<p>
You can prevent libcurl from first trying the EPSV command by setting <a href="#CURLOPT_FTP_USE_EPSV">CURLOPT_FTP_USE_EPSV</a> to False.
</p>
<p>
In some cases, you will prefer to have the server connect back to you for the second connection. This might be when the server is perhaps behind a firewall or something and only allows connections on a single port. libcurl then informs the remote server which IP address and port number to connect to. This is made with the <a href="#CURLOPT_FTPPORT">CURLOPT_FTPPORT</a> option. If you set it to "-", libcurl will use your system's "default IP address". If you want to use a particular IP, you can set the full IP address, a host name to resolve to an IP address or even a local network interface name that libcurl will get the IP address from.
</p>
<p>
When doing the "PORT" approach, libcurl will attempt to use the EPRT and the LPRT before trying PORT, as they work with more protocols. You can disable this behavior by setting <a href="#CURLOPT_FTP_USE_EPRT">CURLOPT_FTP_USE_EPRT</a> to False.
</p>
<h3>Headers Equal Fun</h3>
<p>
Some protocols provide "headers", meta-data separated from the normal data. These headers are by default not included in the normal data stream, but you can make them appear in the data stream by setting <a href="#CURLOPT_HEADER">CURLOPT_HEADER</a> to True.
</p>
<p>
What might be even more useful, is libcurl's ability to separate the headers from the data and thus make the callbacks differ. You can set an entirely separate function to receive the headers, by using <a href="#setHeaderCallback">setHeaderCallback</a>.
</p>
<p>
The headers are passed to the callback function one by one, and you can depend on that fact. It makes it easier for you to add custom header parsers etc.
</p>
<p>
"Headers" for FTP transfers equal all the FTP server responses. They aren't actually true headers, but in this case we pretend they are! ;-)
</p>
<h3>Multiple Transfers Using the multi Interface</h3>
<p>
The easy interface as described in detail in this document is a synchronous interface that transfers one file at a time and doesn't return until its done.
</p>
<p>
The multi interface on the other hand, allows your program to transfer multiple files in both directions at the same time, without forcing you to use multiple threads.
</p>
<p>
To use this interface, you are better off if you first understand the basics of how to use the easy interface. The multi interface is simply a way to make multiple transfers at the same time, by adding up multiple easy handles in to a "multi stack".
</p>
<p>
You create the easy handles you want and you set all the options just like you have been told above, and then you create a <a href="#TCurlMulti">TCurlMulti</a> multi handle and add all those easy handles to that multi handle with <a href="#multiAdd">multiAdd</a>. Or, if you wish you can create a new easy handle and add it to the multi
handle automatically by using <a href="#newEasy">newEasy</a>.
</p>
<p>
When you've added the handles you have for the moment (you can still add new ones at any time), you start the transfers by call <a href="#multiPerform">multiPerform</a>.
</p>
<p>
<a href="#multiPerform">multiPerform</a> is asynchronous. It will only execute as little as possible and then return back control to your program. It is designed to never block. If it returns <a href="#CURLM_CALL_MULTI_PERFORM">CURLM_CALL_MULTI_PERFORM</a> you better call it again soon, as that is a signal that it still has local data to send or remote data to receive.
</p>
<p>
The best usage of this interface is when you do a <a href="#multiSelect">multiSelect</a> on all possible file descriptors or sockets to know when to call libcurl again. This also makes it easy for you to wait and respond to actions on your own application's sockets/handles.
</p>
<p>
When you then call <a href="#multiSelect">multiSelect</a>, it'll return when one of the file handles signal action and you then call <a href="#multiPerform">multiPerform</a> to allow libcurl to do what it wants to do. Take note that libcurl does also feature some time-out code so we advice you to never use very long timeouts on <a href="#multiSelect">multiSelect</a> before you call <a href="#multiPerform">multiPerform</a>, which thus should be called unconditionally every now and then even if none of its file descriptors have signaled ready.
</p>
<p>
If you want to stop the transfer of one of the easy handles in the stack, you can use <a href="#multiRemove">multiRemove</a> to remove individual easy handles. Remember that easy handles should be <a href="#cleanup">cleanup</a>ed.
</p>
<p>
When a transfer within the multi stack has finished, the counter of running transfers (as filled in by <a href="#multiPerform">multiPerform</a>) will decrease. When the number reaches zero, all transfers are done.
</p>
<p>
<a href="#multiInfoRead">multiInfoRead</a> can be used to get information about completed transfers. It then returns the CURLcode for each easy transfer, to allow you to figure out success on each individual transfer.
</p>




